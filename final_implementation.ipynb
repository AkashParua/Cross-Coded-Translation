{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"knowledge_base.pkl\"\n",
    "\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    knowledge_base = pickle.load(file)\n",
    "# loading knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump({}, file)\n",
    "'''\n",
    "# creating knowledge base for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/miniconda3/envs/cleantorch/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/miniconda3/envs/cleantorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration , AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"english_to_hinglish_tokenizer\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"my-t5-hinglish-translator\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUT YOU INPUT TEXT HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT YOUR TEXT HERE\n",
    "sample_passage =\"I had about a 30 minute demo just using this new headset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_passage)\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "nouns = set(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# words that might be used in their english form\n",
    "sus_en_words = set()\n",
    "def translate(sentence):\n",
    "    sentence = \"Translate English to Hinglish : \" + sentence.strip().lower()\n",
    "    print(f'processing {sentence}.........')\n",
    "    input_ids = tokenizer(sentence , return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    output_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'output : {output_sentence}')\n",
    "    words1 = set(sentence.lower().split())\n",
    "    words2 = set(output_sentence.lower().split())\n",
    "\n",
    "    common_words = words1.intersection(words2)\n",
    "    #check whether word is a verb using spacy\n",
    "    for word in common_words:\n",
    "    #checking whether the word is a verb \n",
    "    #too many false positive for verb hence not using it\n",
    "        if word in nouns:\n",
    "            sus_en_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_passage(para):\n",
    "    lines = para.split('.')\n",
    "    for line in lines:\n",
    "        translate(line)\n",
    "    return sus_en_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your sample passage here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Translate English to Hinglish : i had about a 30 minute demo just using this new headset.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/miniconda3/envs/cleantorch/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : kya is new headset ke sath 30 minute demo just using hai\n"
     ]
    }
   ],
   "source": [
    "en_words = process_passage(sample_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demo', 'headset', 'minute'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using google translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "url = \"https://google-translate1.p.rapidapi.com/language/translate/v2\"\n",
    "'''\n",
    "payload = {\n",
    "\t\"q\": \"Hello, world!\",\n",
    "\t\"target\": \"hi\",\n",
    "\t\"source\": \"en\"\n",
    "}\n",
    "'''\n",
    "print(os.getenv('CONTENT'))\n",
    "headers = {\n",
    "\t\"content-type\": os.getenv('CONTENT'),\n",
    "\t\"Accept-Encoding\": os.getenv('ENCODING'),\n",
    "\t\"X-RapidAPI-Key\": os.getenv('RAPIDAPI'),\n",
    "\t\"X-RapidAPI-Host\": os.getenv('HOST')\n",
    "}\n",
    "\n",
    "#response = requests.post(url, data=payload, headers=headers)\n",
    "\n",
    "#print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_knowledgebase(en_words):\n",
    "    for word in en_words:\n",
    "        payload = {\n",
    "            \"q\": word,\n",
    "            \"target\": \"hi\",\n",
    "            \"source\": \"en\"\n",
    "        }\n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        print(response.json())\n",
    "        print(word)\n",
    "        print(response.json()['data']['translations'][0]['translatedText'])\n",
    "        knowledge_base[response.json()['data']['translations'][0]['translatedText']] = word\n",
    "\n",
    "    pickle.dump(knowledge_base, open(file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'You have exceeded the MONTHLY quota for Characters on your current plan, BASIC. Upgrade your plan at https://rapidapi.com/googlecloud/api/google-translate1'}\n",
      "headset\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/akash/Projects/CrossCodedText/final_implementation.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m update_knowledgebase(en_words)\n",
      "\u001b[1;32m/home/akash/Projects/CrossCodedText/final_implementation.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mjson())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(word)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39;49mjson()[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mtranslations\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslatedText\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     knowledge_base[response\u001b[39m.\u001b[39mjson()[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslations\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslatedText\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m word\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akash/Projects/CrossCodedText/final_implementation.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(knowledge_base, \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "update_knowledgebase(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'सड़क': 'road',\n",
       " 'चाय': 'chai',\n",
       " 'ट्रैफ़िक': 'traffic',\n",
       " 'जाम': 'jam',\n",
       " 'खाना': 'food',\n",
       " 'छोटी दुकान': 'stall',\n",
       " 'सुबह': 'morning',\n",
       " 'बुरा अनुभव': 'nightmare',\n",
       " 'यार': 'yaar',\n",
       " 'हत्यारा': 'killer',\n",
       " 'रेलगाड़ी': 'train',\n",
       " 'गली': 'street'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now translating the passage \n",
    "response = requests.post(url, data={\"q\":sample_passage, \"target\": \"hi\", \"source\": \"en\"}, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_response = response.json()['data']['translations'][0]['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अरे भाई! आज सुबह लोकल ट्रेन पूरी तरह से एक दुःस्वप्न थी। यह खचाखच भरा हुआ था, और मैं मुश्किल से अंदर जा पाया। और आप इस पर विश्वास नहीं करेंगे, लेकिन मेरे कार्यालय जाने के रास्ते में दादर में भारी ट्रैफिक जाम था। ऐसा लगता है कि सभी ने एक ही समय पर सड़क पर उतरने का फैसला किया है, यार। लेकिन, तुम्हें पता है, यहां का स्ट्रीट फूड जानलेवा है। स्टेशन के पास स्टॉल पर मुझे कुछ अद्भुत वड़ा पाव और कटिंग चाय मिली। उस स्वाद को हरा नहीं सकते, है ना? और मेरे सहकर्मी इस शुक्रवार को बांद्रा में एक नए रूफटॉप बार में एक साथ मिलने की योजना बना रहे हैं। यदि आप स्वतंत्र हैं तो आपको पूरी तरह से हमसे जुड़ना चाहिए। यह एक धमाका होने वाला है'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "सुबह\n",
      "जाम\n",
      "सड़क\n",
      "चाय\n"
     ]
    }
   ],
   "source": [
    "for word in response.json()['data']['translations'][0]['translatedText'].split():\n",
    "    if word in knowledge_base.keys():\n",
    "        print(word)\n",
    "        edited_response = edited_response.replace(word, knowledge_base[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अरे भाई! आज morning लोकल ट्रेन पूरी तरह से एक दुःस्वप्न थी। यह खचाखच भरा हुआ था, और मैं मुश्किल से अंदर जा पाया। और आप इस पर विश्वास नहीं करेंगे, लेकिन मेरे कार्यालय जाने के रास्ते में दादर में भारी ट्रैफिक jam था। ऐसा लगता है कि सभी ने एक ही समय पर road पर उतरने का फैसला किया है, यार। लेकिन, तुम्हें पता है, यहां का स्ट्रीट फूड जानलेवा है। स्टेशन के पास स्टॉल पर मुझे कुछ अद्भुत वड़ा पाव और कटिंग chai मिली। उस स्वाद को हरा नहीं सकते, है ना? और मेरे सहकर्मी इस शुक्रवार को बांद्रा में एक नए रूफटॉप बार में एक साथ मिलने की योजना बना रहे हैं। यदि आप स्वतंत्र हैं तो आपको पूरी तरह से हमसे जुड़ना चाहिए। यह एक धमाका होने वाला है'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleantorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
